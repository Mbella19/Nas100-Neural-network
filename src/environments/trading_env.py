"""
Gymnasium Trading Environment for the Sniper Agent.

Features:
- Multi-Discrete action space: [Direction (3), Size (4)]
- Frozen Market Analyst provides context vectors
- Reward shaping: PnL, transaction costs, FOMO penalty, chop avoidance
- Normalized observations (prevents scale inconsistencies)

Optimized for M2 Silicon with all float32 operations.
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import torch
from typing import Dict, Tuple, Optional, Any
import pandas as pd
import gc


class TradingEnv(gym.Env):
    """
    Trading environment for the PPO Sniper Agent.

    Action Space:
        Multi-Discrete([3, 4]):
        - Direction: 0=Flat/Exit, 1=Long, 2=Short
        - Size: 0=0.25x, 1=0.5x, 2=0.75x, 3=1.0x

    Observation Space:
        Box containing:
        - Context vector from frozen Analyst (context_dim)
        - Position state: [position, entry_price_norm, unrealized_pnl_norm]
        - Market features: [atr, chop, adx, regime, sma_distance]

    Reward:
        Base PnL (pips) × position_size
        - Transaction cost when opening
        - FOMO penalty when flat during momentum
        - Chop penalty when holding in ranging market
    """

    metadata = {'render_modes': ['human']}

    # Position sizing multipliers
    POSITION_SIZES = (0.25, 0.5, 0.75, 1.0)

    def __init__(
        self,
        data_15m: np.ndarray,
        data_1h: np.ndarray,
        data_4h: np.ndarray,
        close_prices: np.ndarray,
        market_features: np.ndarray,
        analyst_model: Optional[torch.nn.Module] = None,
        context_dim: int = 64,
        lookback_15m: int = 48,
        lookback_1h: int = 24,
        lookback_4h: int = 12,
        spread_pips: float = 1.5,
        fomo_penalty: float = -0.2,   # Further reduced: now ~2 pip equivalent (was -1.0 = 10 pips)
        chop_penalty: float = -0.1,   # Further reduced: now ~1 pip equivalent per bar (was -0.5 = 5 pips)
        fomo_threshold_atr: float = 2.0,
        chop_threshold: float = 60.0,
        max_steps: int = 2000,
        reward_scaling: float = 0.1,  # Scale PnL rewards to balance with penalties
        device: Optional[torch.device] = None,
        market_feat_mean: Optional[np.ndarray] = None,  # Pre-computed from training data
        market_feat_std: Optional[np.ndarray] = None,    # Pre-computed from training data
        pre_windowed: bool = True,  # FIXED: If True, data is already windowed (start_idx=0)
        # Risk Management
        sl_atr_multiplier: float = 1.5, # Stop Loss = ATR * multiplier
        tp_atr_multiplier: float = 3.0, # Take Profit = ATR * multiplier
        use_stop_loss: bool = True,     # Enable/disable stop-loss
        use_take_profit: bool = True,   # Enable/disable take-profit
        # Regime-balanced sampling
        regime_labels: Optional[np.ndarray] = None,  # 0=Bullish, 1=Ranging, 2=Bearish
        use_regime_sampling: bool = True,  # Sample episodes balanced across regimes
        # Volatility Sizing
        volatility_sizing: bool = True,  # Scale position size inversely to ATR
        risk_pips_target: float = 15.0   # Reference risk for normalization (e.g. 15 pips)
    ):
        """
        Initialize the trading environment.

        Args:
            data_15m: 15-minute feature data [num_samples, lookback_15m, features]
            data_1h: 1-hour feature data [num_samples, lookback_1h, features]
            data_4h: 4-hour feature data [num_samples, lookback_4h, features]
            close_prices: Close prices for PnL calculation [num_samples]
            market_features: Additional features [num_samples, n_features]
                            Expected: [atr, chop, adx, regime, sma_distance]
            analyst_model: Frozen Market Analyst for context generation
            context_dim: Dimension of context vector
            lookback_*: Lookback windows for each timeframe
            spread_pips: Transaction cost in pips
            fomo_penalty: Penalty for being flat during momentum
            chop_penalty: Penalty for holding in ranging market
            fomo_threshold_atr: ATR multiplier for FOMO detection
            chop_threshold: Choppiness index threshold
            max_steps: Maximum steps per episode
            reward_scaling: Scale factor for PnL rewards (0.1 = ±20 pips becomes ±2.0)
                           This balances PnL with penalties for "Sniper" behavior.
            device: Torch device for analyst inference
        """
        super().__init__()

        # Store data (ensure float32)
        self.data_15m = data_15m.astype(np.float32)
        self.data_1h = data_1h.astype(np.float32)
        self.data_4h = data_4h.astype(np.float32)
        self.close_prices = close_prices.astype(np.float32)
        self.market_features = market_features.astype(np.float32)

        # Analyst model
        self.analyst = analyst_model
        self.device = device or torch.device('cpu')
        self.context_dim = context_dim

        # Lookback windows
        self.lookback_15m = lookback_15m
        self.lookback_1h = lookback_1h
        self.lookback_4h = lookback_4h

        # Trading parameters
        self.spread_pips = spread_pips
        self.fomo_penalty = fomo_penalty
        self.chop_penalty = chop_penalty
        self.fomo_threshold_atr = fomo_threshold_atr
        self.chop_threshold = chop_threshold
        self.max_steps = max_steps
        self.reward_scaling = reward_scaling  # Scale PnL to balance with penalties

        # Risk Management - Stop-Loss and Take-Profit
        self.sl_atr_multiplier = sl_atr_multiplier
        self.tp_atr_multiplier = tp_atr_multiplier
        self.use_stop_loss = use_stop_loss
        self.use_stop_loss = use_stop_loss
        self.use_take_profit = use_take_profit
        
        # Volatility Sizing
        self.volatility_sizing = volatility_sizing
        self.risk_pips_target = risk_pips_target

        # Calculate valid range FIRST (needed for regime indices)
        # FIXED: If pre_windowed=True, data is already trimmed by prepare_env_data
        # so start_idx should be 0 (no double offset)
        if pre_windowed:
            self.start_idx = 0
        else:
            # Only compute start_idx if using raw DataFrames (create_env_from_dataframes)
            self.start_idx = max(lookback_15m, lookback_1h * 4, lookback_4h * 16)
        
        self.end_idx = len(close_prices) - 1
        self.n_samples = self.end_idx - self.start_idx
        
        # Regime-balanced sampling (AFTER start_idx/end_idx are set)
        self.use_regime_sampling = use_regime_sampling and regime_labels is not None
        if regime_labels is not None:
            self.regime_labels = regime_labels.astype(np.int32)
            # Pre-compute indices for each regime (0=Bullish, 1=Ranging, 2=Bearish)
            self.regime_indices = {
                0: np.where(self.regime_labels == 0)[0],  # Bullish
                1: np.where(self.regime_labels == 1)[0],  # Ranging
                2: np.where(self.regime_labels == 2)[0],  # Bearish
            }
            # Filter to valid range for episode starts
            max_start = max(self.start_idx + 1, self.end_idx - max_steps)
            for regime in self.regime_indices:
                valid = self.regime_indices[regime]
                valid = valid[(valid >= self.start_idx) & (valid < max_start)]
                self.regime_indices[regime] = valid
            # Log regime distribution
            print(f"Regime sampling enabled: Bullish={len(self.regime_indices[0])}, "
                  f"Ranging={len(self.regime_indices[1])}, Bearish={len(self.regime_indices[2])}")
        else:
            self.regime_labels = None
            self.regime_indices = None

        # Action space: Multi-Discrete([direction, size])
        # Direction: 0=Flat, 1=Long, 2=Short
        # Size: 0=0.25, 1=0.5, 2=0.75, 3=1.0
        self.action_space = spaces.MultiDiscrete([3, 4])

        # Observation space
        # Context vector + position state (3) + market features (5)
        n_market_features = market_features.shape[1] if len(market_features.shape) > 1 else 5
        obs_dim = context_dim + 3 + n_market_features
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(obs_dim,),
            dtype=np.float32
        )

        # CRITICAL: Normalize market features to prevent scale inconsistencies!
        # Market features (ATR ~0.001, CHOP 0-100, ADX 0-100) have vastly different scales.
        # FIXED: Use pre-computed stats from training data to prevent look-ahead bias.
        if market_feat_mean is not None and market_feat_std is not None:
            # Use pre-computed statistics (no look-ahead bias)
            self.market_feat_mean = market_feat_mean.astype(np.float32)
            self.market_feat_std = market_feat_std.astype(np.float32)
        elif len(market_features.shape) > 1 and market_features.shape[1] > 0:
            # Fallback: compute from provided data (should only be used with training data)
            self.market_feat_mean = market_features.mean(axis=0).astype(np.float32)
            self.market_feat_std = market_features.std(axis=0).astype(np.float32)
            # Prevent division by zero for constant features
            self.market_feat_std = np.where(self.market_feat_std > 1e-8,
                                           self.market_feat_std,
                                           1.0).astype(np.float32)
        else:
            self.market_feat_mean = None
            self.market_feat_std = None

        # Episode state
        self.current_idx = self.start_idx
        self.position = 0  # -1: Short, 0: Flat, 1: Long
        self.position_size = 0.0
        self.entry_price = 0.0
        self.steps = 0
        self.total_pnl = 0.0
        self.trades = []
        self.prev_unrealized_pnl = 0.0  # Track for continuous PnL rewards

        # Precompute context vectors if analyst is provided
        self._precomputed_contexts = None
        if self.analyst is not None:
            self._precompute_contexts()

    def _precompute_contexts(self):
        """Precompute all context vectors for efficiency."""
        if self.analyst is None:
            return

        print("Precomputing context vectors...")
        self.analyst.eval()

        contexts = []
        batch_size = 64

        with torch.no_grad():
            for i in range(0, self.n_samples, batch_size):
                end_i = min(i + batch_size, self.n_samples)
                actual_indices = range(self.start_idx + i, self.start_idx + end_i)

                # Get batch data
                batch_15m = torch.tensor(
                    self.data_15m[list(actual_indices)],
                    device=self.device,
                    dtype=torch.float32
                )
                batch_1h = torch.tensor(
                    self.data_1h[list(actual_indices)],
                    device=self.device,
                    dtype=torch.float32
                )
                batch_4h = torch.tensor(
                    self.data_4h[list(actual_indices)],
                    device=self.device,
                    dtype=torch.float32
                )

                # Get context
                context = self.analyst.get_context(batch_15m, batch_1h, batch_4h)
                contexts.append(context.cpu().numpy())

                # Memory cleanup
                del batch_15m, batch_1h, batch_4h, context
                if i % (batch_size * 10) == 0:
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                    gc.collect()

        self._precomputed_contexts = np.vstack(contexts).astype(np.float32)
        print(f"Precomputed {len(self._precomputed_contexts)} context vectors")

    def _get_context(self, idx: int) -> np.ndarray:
        """Get context vector for current index."""
        if self._precomputed_contexts is not None:
            # Use precomputed
            context_idx = idx - self.start_idx
            if 0 <= context_idx < len(self._precomputed_contexts):
                return self._precomputed_contexts[context_idx]

        if self.analyst is not None:
            # Compute on-the-fly
            with torch.no_grad():
                x_15m = torch.tensor(
                    self.data_15m[idx:idx+1],
                    device=self.device,
                    dtype=torch.float32
                )
                x_1h = torch.tensor(
                    self.data_1h[idx:idx+1],
                    device=self.device,
                    dtype=torch.float32
                )
                x_4h = torch.tensor(
                    self.data_4h[idx:idx+1],
                    device=self.device,
                    dtype=torch.float32
                )
                context = self.analyst.get_context(x_15m, x_1h, x_4h)
                return context.cpu().numpy().flatten()

        # No analyst - return zeros
        return np.zeros(self.context_dim, dtype=np.float32)

    def _get_observation(self) -> np.ndarray:
        """Construct observation vector."""
        # Context vector
        context = self._get_context(self.current_idx)

        # Position state
        current_price = self.close_prices[self.current_idx]
        atr = self.market_features[self.current_idx, 0] if len(self.market_features.shape) > 1 else 1.0

        # Normalize entry price and unrealized PnL
        # CRITICAL FIX: Use floor for ATR to prevent division by near-zero
        atr_safe = max(atr, 1e-6)
        if self.position != 0:
            entry_price_norm = (self.entry_price - current_price) / (atr_safe * 100)
            # Clip to prevent extreme values
            entry_price_norm = np.clip(entry_price_norm, -10.0, 10.0)
            unrealized_pnl = self._calculate_unrealized_pnl()
            unrealized_pnl_norm = unrealized_pnl / 100  # Normalize by 100 pips
        else:
            entry_price_norm = 0.0
            unrealized_pnl_norm = 0.0

        position_state = np.array([
            float(self.position),
            entry_price_norm,
            unrealized_pnl_norm
        ], dtype=np.float32)

        # Market features (NORMALIZED to prevent scale inconsistencies)
        if len(self.market_features.shape) > 1:
            market_feat_raw = self.market_features[self.current_idx]
            # Apply Z-score normalization
            if self.market_feat_mean is not None and self.market_feat_std is not None:
                market_feat = ((market_feat_raw - self.market_feat_mean) /
                              self.market_feat_std).astype(np.float32)
            else:
                market_feat = market_feat_raw
        else:
            market_feat = np.zeros(5, dtype=np.float32)

        # Combine all (all components now on similar scales)
        obs = np.concatenate([context, position_state, market_feat])

        return obs.astype(np.float32)

    def _calculate_unrealized_pnl(self) -> float:
        """Calculate unrealized PnL in pips."""
        if self.position == 0:
            return 0.0

        current_price = self.close_prices[self.current_idx]
        pip_value = 0.0001  # EURUSD pip

        if self.position == 1:  # Long
            pnl_pips = (current_price - self.entry_price) / pip_value
        else:  # Short
            pnl_pips = (self.entry_price - current_price) / pip_value

        return pnl_pips * self.position_size

    def _check_stop_loss_take_profit(self) -> Tuple[float, dict]:
        """
        Check and execute stop-loss or take-profit if triggered.

        This method is called BEFORE the agent's action to enforce risk management.
        Stop-loss cuts losing positions early to prevent catastrophic losses.
        Take-profit locks in gains to improve risk/reward ratio.

        Returns:
            Tuple of (reward, info_dict) if triggered, (0.0, {}) otherwise
        """
        # No position = nothing to check
        if self.position == 0:
            return 0.0, {}

        # Get current price
        current_price = self.close_prices[self.current_idx]
        pip_value = 0.0001

        # Calculate raw pips (before position size adjustment)
        if self.position == 1:  # Long
            raw_pips = (current_price - self.entry_price) / pip_value
        else:  # Short
            raw_pips = (self.entry_price - current_price) / pip_value

        # Get ATR for dynamic thresholds
        if len(self.market_features.shape) > 1:
            atr = self.market_features[self.current_idx, 0]
        else:
            atr = 0.001  # Default fallback

        # Calculate dynamic thresholds
        sl_pips_threshold = (atr * self.sl_atr_multiplier) / 0.0001
        tp_pips_threshold = (atr * self.tp_atr_multiplier) / 0.0001
        
        # Enforce minimums (e.g. 5 pips) to prevent noise stop-outs
        sl_pips_threshold = max(sl_pips_threshold, 5.0)
        tp_pips_threshold = max(tp_pips_threshold, 5.0)

        # Check stop-loss (loss exceeds threshold)
        if self.use_stop_loss and raw_pips < -sl_pips_threshold:
            # Calculate realized PnL
            pnl = self._calculate_unrealized_pnl()
            
            # Close position
            self.total_pnl += pnl
            self.trades.append({
                'entry': self.entry_price,
                'exit': current_price,
                'direction': self.position,
                'size': self.position_size,
                'pnl': pnl,
                'close_reason': 'stop_loss'
            })
            
            # Reset
            self.position = 0
            self.position_size = 0.0
            self.entry_price = 0.0
            self.prev_unrealized_pnl = 0.0
            
            return pnl, {
                'stop_loss_triggered': True,
                'trade_closed': True,
                'close_reason': 'stop_loss',
                'pnl': pnl
            }

        # Check take-profit (profit exceeds threshold)
        if self.use_take_profit and raw_pips > tp_pips_threshold:
            pnl = self._calculate_unrealized_pnl()
            
            self.total_pnl += pnl
            self.trades.append({
                'entry': self.entry_price,
                'exit': current_price,
                'direction': self.position,
                'size': self.position_size,
                'pnl': pnl,
                'close_reason': 'take_profit'
            })
            
            self.position = 0
            self.position_size = 0.0
            self.entry_price = 0.0
            self.prev_unrealized_pnl = 0.0
            
            return pnl, {
                'take_profit_triggered': True,
                'trade_closed': True,
                'close_reason': 'take_profit',
                'pnl': pnl
            }

        return 0.0, {}

    def _execute_action(self, action: np.ndarray) -> Tuple[float, dict]:
        """
        Execute trading action and calculate reward.

        Returns:
            Tuple of (reward, info_dict)
        """
        direction = action[0]  # 0=Flat, 1=Long, 2=Short
        size_idx = action[1]   # 0-3
        base_size = self.POSITION_SIZES[size_idx]
        
        # Volatility Sizing: Adjust position size so that risk is constant
        # If ATR is high (wide SL), size should be small.
        # If ATR is low (tight SL), size should be large.
        # Target: SL distance * Position Size ~= Constant
        new_size = base_size
        
        if self.volatility_sizing and len(self.market_features.shape) > 1:
            atr = self.market_features[self.current_idx, 0]
            # Calculate SL pips for this ATR
            sl_pips = (atr * self.sl_atr_multiplier) / 0.0001
            sl_pips = max(sl_pips, 5.0) # Minimum 5 pips
            
            # Calculate volatility scalar
            # Example: Target=15, SL=30 (High Vol) -> Scalar = 0.5 -> Size halved
            # Example: Target=15, SL=7.5 (Low Vol) -> Scalar = 2.0 -> Size doubled
            vol_scalar = self.risk_pips_target / sl_pips
            
            # Apply scalar to base size
            new_size = base_size * vol_scalar
            
            # Clip to reasonable limits (e.g. 0.1x to 5.0x) to prevent extreme leverage
            new_size = np.clip(new_size, 0.1, 5.0)

        reward = 0.0
        info = {
            'trade_opened': False,
            'trade_closed': False,
            'pnl': 0.0
        }

        current_price = self.close_prices[self.current_idx]
        prev_price = self.close_prices[self.current_idx - 1] if self.current_idx > 0 else current_price

        # Get market conditions
        if len(self.market_features.shape) > 1:
            atr = self.market_features[self.current_idx, 0]
            chop = self.market_features[self.current_idx, 1]
        else:
            atr = 0.001
            chop = 50.0

        # Calculate price move for FOMO detection
        price_move = abs(current_price - prev_price)
        pip_value = 0.0001

        # Handle position changes
        # Reward structure: Continuous pnl_delta rewards every step.
        # On exit: The final delta (last price leg) is captured BEFORE resetting position
        # to ensure the agent receives complete reward signal for the entire trade.
        if direction == 0:  # Flat/Exit
            if self.position != 0:
                # CRITICAL: Calculate final delta BEFORE resetting position
                # This captures the last price leg that would otherwise be missed
                final_unrealized = self._calculate_unrealized_pnl()
                final_delta = final_unrealized - self.prev_unrealized_pnl
                reward += final_delta * self.reward_scaling  # Capture final leg!

                # Direction bonus: reward correct direction predictions
                # FIXED: Scale by position_size and reward_scaling to match PnL scaling
                if final_unrealized > 0:
                    reward += 5.0 * self.position_size * self.reward_scaling  # ~5 pip bonus scaled
                    info['direction_bonus'] = True

                # Record trade statistics
                info['trade_closed'] = True
                info['pnl'] = final_unrealized  # Unscaled for tracking
                info['pnl_delta'] = final_delta
                self.total_pnl += final_unrealized
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': current_price,
                    'direction': self.position,
                    'size': self.position_size,
                    'pnl': final_unrealized
                })

                # NOW reset position state
                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0

        elif direction == 1:  # Long
            if self.position == -1:  # Close short first
                # CRITICAL: Calculate final delta BEFORE resetting position
                final_unrealized = self._calculate_unrealized_pnl()
                final_delta = final_unrealized - self.prev_unrealized_pnl
                reward += final_delta * self.reward_scaling  # Capture final leg!

                # Direction bonus: reward correct direction predictions
                # FIXED: Scale by position_size and reward_scaling to match PnL scaling
                if final_unrealized > 0:
                    reward += 5.0 * self.position_size * self.reward_scaling  # ~5 pip bonus scaled
                    info['direction_bonus'] = True

                info['trade_closed'] = True
                info['pnl'] = final_unrealized
                info['pnl_delta'] = final_delta
                self.total_pnl += final_unrealized
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': current_price,
                    'direction': -1,
                    'size': self.position_size,
                    'pnl': final_unrealized
                })

                # NOW reset position state before opening new one
                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0

            if self.position != 1:  # Open long
                self.position = 1
                self.position_size = new_size
                self.entry_price = current_price
                reward -= self.spread_pips * new_size * self.reward_scaling  # Spread cost
                # CRITICAL FIX: Include spread in total_pnl to match backtest accounting
                self.total_pnl -= self.spread_pips * new_size
                info['trade_opened'] = True

        elif direction == 2:  # Short
            if self.position == 1:  # Close long first
                # CRITICAL: Calculate final delta BEFORE resetting position
                final_unrealized = self._calculate_unrealized_pnl()
                final_delta = final_unrealized - self.prev_unrealized_pnl
                reward += final_delta * self.reward_scaling  # Capture final leg!

                # Direction bonus: reward correct direction predictions
                # Scaled by position_size * reward_scaling for consistency with backtest
                if final_unrealized > 0:
                    reward += 5.0 * self.position_size * self.reward_scaling  # ~5 pip bonus scaled
                    info['direction_bonus'] = True

                info['trade_closed'] = True
                info['pnl'] = final_unrealized
                info['pnl_delta'] = final_delta
                self.total_pnl += final_unrealized
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': current_price,
                    'direction': 1,
                    'size': self.position_size,
                    'pnl': final_unrealized
                })

                # NOW reset position state before opening new one
                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0

            if self.position != -1:  # Open short
                self.position = -1
                self.position_size = new_size
                self.entry_price = current_price
                reward -= self.spread_pips * new_size * self.reward_scaling  # Spread cost
                # CRITICAL FIX: Include spread in total_pnl to match backtest accounting
                self.total_pnl -= self.spread_pips * new_size
                info['trade_opened'] = True

        # Continuous PnL feedback: reward based on CHANGE in unrealized PnL each step.
        # This prevents the "death spiral" where holding in chop accumulates penalties
        # with no offsetting reward until exit.
        # FIXED: This is now the ONLY source of PnL reward (no duplicate on exit).
        current_unrealized_pnl = self._calculate_unrealized_pnl()
        pnl_delta = current_unrealized_pnl - self.prev_unrealized_pnl

        if self.position != 0:
            # Add the CHANGE in unrealized PnL to reward each step
            reward += pnl_delta * self.reward_scaling
            info['unrealized_pnl'] = current_unrealized_pnl
            info['pnl_delta'] = pnl_delta

        # Update for next step
        self.prev_unrealized_pnl = current_unrealized_pnl

        # FOMO penalty: flat during high momentum move
        if self.position == 0:
            if price_move > self.fomo_threshold_atr * atr:
                reward += self.fomo_penalty
                info['fomo_triggered'] = True

        # Chop penalty: holding position in ranging market
        if self.position != 0 and chop > self.chop_threshold:
            reward += self.chop_penalty
            info['chop_triggered'] = True

        return reward, info

    def reset(
        self,
        seed: Optional[int] = None,
        options: Optional[dict] = None
    ) -> Tuple[np.ndarray, dict]:
        """Reset the environment for a new episode."""
        super().reset(seed=seed)

        # Random starting point
        if options and 'start_idx' in options:
            self.current_idx = options['start_idx']
        elif self.use_regime_sampling and self.regime_indices is not None:
            # REGIME-BALANCED SAMPLING: Equal probability for each regime
            # This prevents directional bias from unbalanced training data
            available_regimes = [r for r in [0, 1, 2] if len(self.regime_indices[r]) > 0]
            if len(available_regimes) > 0:
                # Randomly pick a regime
                chosen_regime = self.np_random.choice(available_regimes)
                # Randomly pick a starting index from that regime
                regime_idx = self.np_random.integers(0, len(self.regime_indices[chosen_regime]))
                self.current_idx = self.regime_indices[chosen_regime][regime_idx]
            else:
                # Fallback to random if no regime indices available
                max_start = max(self.start_idx + 1, self.end_idx - self.max_steps)
                self.current_idx = self.np_random.integers(self.start_idx, max_start)
        else:
            # FIXED: Ensure valid range for random start
            max_start = max(self.start_idx + 1, self.end_idx - self.max_steps)
            self.current_idx = self.np_random.integers(
                self.start_idx,
                max_start
            )

        # Reset state
        self.position = 0
        self.position_size = 0.0
        self.entry_price = 0.0
        self.steps = 0
        self.total_pnl = 0.0
        self.trades = []
        self.prev_unrealized_pnl = 0.0  # Reset for continuous PnL tracking

        return self._get_observation(), {}

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:
        """
        Execute one step in the environment.

        Args:
            action: [direction, size] array

        Returns:
            observation, reward, terminated, truncated, info
        """
        # FIRST: Check stop-loss/take-profit BEFORE agent action
        # This enforces risk management regardless of what the agent wants to do
        sl_tp_reward, sl_tp_info = self._check_stop_loss_take_profit()

        # THEN: Execute agent's action (which may open new positions or do nothing)
        action_reward, action_info = self._execute_action(action)

        # Combine rewards and info
        reward = sl_tp_reward + action_reward
        info = {**action_info, **sl_tp_info}  # SL/TP info takes precedence

        # Move to next step
        self.current_idx += 1
        self.steps += 1

        # Check termination
        terminated = self.current_idx >= self.end_idx
        truncated = self.steps >= self.max_steps

        # Get new observation
        obs = self._get_observation()

        # Add episode info
        info['step'] = self.steps
        info['position'] = self.position
        info['total_pnl'] = self.total_pnl
        info['n_trades'] = len(self.trades)
        # Pass trades list for win rate calculation (only on episode end to save memory)
        if terminated or truncated:
            info['trades'] = self.trades.copy()

        return obs, reward, terminated, truncated, info

    def render(self, mode: str = 'human'):
        """Render current state."""
        if mode == 'human':
            print(f"Step: {self.steps}, Position: {self.position}, "
                  f"Size: {self.position_size:.2f}, PnL: {self.total_pnl:.2f} pips")

    def close(self):
        """Clean up resources."""
        del self._precomputed_contexts
        gc.collect()


def create_env_from_dataframes(
    df_15m: 'pd.DataFrame',
    df_1h: 'pd.DataFrame',
    df_4h: 'pd.DataFrame',
    analyst_model: Optional[torch.nn.Module] = None,
    feature_cols: Optional[list] = None,
    config: Optional[object] = None,
    device: Optional[torch.device] = None
) -> TradingEnv:
    """
    Factory function to create TradingEnv from DataFrames.
    
    FIXED: 1H and 4H data now correctly subsampled from the aligned 15m index.

    Args:
        df_15m: 15-minute DataFrame with features
        df_1h: 1-hour DataFrame with features (aligned to 15m index)
        df_4h: 4-hour DataFrame with features (aligned to 15m index)
        analyst_model: Trained Market Analyst
        feature_cols: Feature columns to use
        config: TradingConfig object
        device: Torch device for analyst inference

    Returns:
        TradingEnv instance
    """
    import pandas as pd

    if feature_cols is None:
        feature_cols = ['open', 'high', 'low', 'close', 'atr',
                       'pinbar', 'engulfing', 'doji', 'ema_trend', 'regime']

    # Get default config values
    lookback_15m = 48
    lookback_1h = 24
    lookback_4h = 12

    if config is not None:
        lookback_15m = getattr(config, 'lookback_15m', 48)
        lookback_1h = getattr(config, 'lookback_1h', 24)
        lookback_4h = getattr(config, 'lookback_4h', 12)

    # Subsampling ratios: how many 15m bars per higher TF bar
    subsample_1h = 4   # 4 x 15m = 1H
    subsample_4h = 16  # 16 x 15m = 4H

    # Calculate valid range - need enough indices for subsampled lookback
    start_idx = max(lookback_15m, lookback_1h * subsample_1h, lookback_4h * subsample_4h)
    n_samples = len(df_15m) - start_idx

    # Get feature arrays
    features_15m = df_15m[feature_cols].values.astype(np.float32)
    features_1h = df_1h[feature_cols].values.astype(np.float32)
    features_4h = df_4h[feature_cols].values.astype(np.float32)

    # Create windows for each timeframe
    data_15m = np.zeros((n_samples, lookback_15m, len(feature_cols)), dtype=np.float32)
    data_1h = np.zeros((n_samples, lookback_1h, len(feature_cols)), dtype=np.float32)
    data_4h = np.zeros((n_samples, lookback_4h, len(feature_cols)), dtype=np.float32)

    for i in range(n_samples):
        actual_idx = start_idx + i
        # 15m: direct indexing
        data_15m[i] = features_15m[actual_idx - lookback_15m:actual_idx]
        
        # FIXED: 1H - subsample every 4th bar from aligned data
        idx_range_1h = list(range(actual_idx - lookback_1h * subsample_1h, actual_idx, subsample_1h))
        data_1h[i] = features_1h[idx_range_1h]
        
        # FIXED: 4H - subsample every 16th bar from aligned data
        idx_range_4h = list(range(actual_idx - lookback_4h * subsample_4h, actual_idx, subsample_4h))
        data_4h[i] = features_4h[idx_range_4h]

    # Close prices for PnL
    close_prices = df_15m['close'].values[start_idx:start_idx + n_samples].astype(np.float32)

    # Market features for reward shaping
    market_cols = ['atr', 'chop', 'adx', 'regime', 'sma_distance']
    available_cols = [c for c in market_cols if c in df_15m.columns]
    market_features = df_15m[available_cols].values[start_idx:start_idx + n_samples].astype(np.float32)

    return TradingEnv(
        data_15m=data_15m,
        data_1h=data_1h,
        data_4h=data_4h,
        close_prices=close_prices,
        market_features=market_features,
        analyst_model=analyst_model,
        lookback_15m=lookback_15m,
        lookback_1h=lookback_1h,
        lookback_4h=lookback_4h,
        device=device
    )
