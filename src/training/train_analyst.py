"""
Training script for the Market Analyst (supervised learning).

Trains the Analyst to classify smoothed future returns across
multiple timeframes (5-class directional buckets). After training,
the model is frozen for use with the RL agent.

Memory-optimized for Apple M2 Silicon.

Features:
- Comprehensive logging with TrainingLogger
- Train/Val accuracy and direction accuracy tracking
- Detailed visualizations of training progress
- Memory monitoring and gradient statistics
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Tuple, Optional, List
from tqdm import tqdm
import gc
import time

from ..models.analyst import MarketAnalyst, create_analyst
from ..data.features import (
    create_smoothed_target, 
    create_return_classes, 
    add_market_sessions,
    detect_fractals,
    detect_structure_breaks
)
from ..utils.logging_config import TrainingLogger, get_logger
from ..utils.metrics import (
    MetricsTracker,
    calculate_classification_metrics,
    compute_gradient_norm
)
from ..utils.visualization import TrainingVisualizer

logger = get_logger(__name__)


class MultiTimeframeDataset(Dataset):
    """
    Dataset for multi-timeframe analyst training.

    Each sample contains:
    - 15m features with lookback window
    - 1H features with lookback window (subsampled from aligned 15m index)
    - 4H features with lookback window (subsampled from aligned 15m index)
    - Smoothed future return target
    
    FIXED: 1H and 4H lookbacks now correctly subsample from the aligned data
    to get the proper temporal coverage (24 hours of 1H = 24 candles, not 24 indices).
    """

    def __init__(
        self,
        df_15m: pd.DataFrame,
        df_1h: pd.DataFrame,
        df_4h: pd.DataFrame,
        feature_cols: List[str],
        target: pd.Series,
        class_labels: pd.Series,
        lookback_15m: int = 48,
        lookback_1h: int = 24,
        lookback_4h: int = 12
    ):
        """
        Args:
            df_15m: 15-minute DataFrame
            df_1h: 1-hour DataFrame (aligned to 15m index via ffill)
            df_4h: 4-hour DataFrame (aligned to 15m index via ffill)
            feature_cols: Feature columns to use
            target: Smoothed future return target
            lookback_15m: Number of 15m candles (48 = 12 hours)
            lookback_1h: Number of 1H candles to look back (24 = 24 hours)
            lookback_4h: Number of 4H candles to look back (12 = 48 hours)
        """
        self.lookback_15m = lookback_15m
        self.lookback_1h = lookback_1h
        self.lookback_4h = lookback_4h
        
        # Subsampling ratios: how many 15m bars per higher TF bar
        self.subsample_1h = 4   # 4 x 15m = 1H
        self.subsample_4h = 16  # 16 x 15m = 4H

        # Get feature matrices
        self.features_15m = df_15m[feature_cols].values.astype(np.float32)
        self.features_1h = df_1h[feature_cols].values.astype(np.float32)
        self.features_4h = df_4h[feature_cols].values.astype(np.float32)
        self.targets = target.values.astype(np.float32)
        self.class_labels = class_labels.values.astype(np.float32)

        # FIXED: Calculate start index based on actual temporal coverage needed
        # For 1H lookback: need lookback_1h * 4 indices (since data is aligned to 15m)
        # For 4H lookback: need lookback_4h * 16 indices
        self.start_idx = max(
            lookback_15m,
            lookback_1h * self.subsample_1h,
            lookback_4h * self.subsample_4h
        )
        self.valid_mask = ~np.isnan(self.class_labels[self.start_idx:])
        self.valid_indices = np.where(self.valid_mask)[0] + self.start_idx

        logger.info(f"Dataset created with {len(self.valid_indices)} valid samples")
        logger.info(f"  15m lookback: {lookback_15m} bars = {lookback_15m * 15 / 60:.1f} hours")
        logger.info(f"  1H lookback: {lookback_1h} bars = {lookback_1h} hours (using {lookback_1h * self.subsample_1h} aligned indices)")
        logger.info(f"  4H lookback: {lookback_4h} bars = {lookback_4h * 4} hours (using {lookback_4h * self.subsample_4h} aligned indices)")

    def __len__(self) -> int:
        return len(self.valid_indices)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        actual_idx = self.valid_indices[idx]

        # Get 15m lookback window (direct indexing)
        x_15m = self.features_15m[actual_idx - self.lookback_15m:actual_idx]
        
        # FIXED: Get 1H lookback by subsampling every 4th bar from aligned data
        # This gives us lookback_1h actual 1H candles worth of data
        idx_range_1h = range(actual_idx - self.lookback_1h * self.subsample_1h, actual_idx, self.subsample_1h)
        x_1h = self.features_1h[list(idx_range_1h)]
        
        # FIXED: Get 4H lookback by subsampling every 16th bar from aligned data
        # This gives us lookback_4h actual 4H candles worth of data
        idx_range_4h = range(actual_idx - self.lookback_4h * self.subsample_4h, actual_idx, self.subsample_4h)
        x_4h = self.features_4h[list(idx_range_4h)]

        # Target
        y = self.class_labels[actual_idx]

        return (
            torch.tensor(x_15m, dtype=torch.float32),
            torch.tensor(x_1h, dtype=torch.float32),
            torch.tensor(x_4h, dtype=torch.float32),
            torch.tensor(int(y), dtype=torch.long)
        )


class FocalLoss(nn.Module):
    """
    Focal Loss for multi-class classification.
    
    FL(p_t) = -α_t * (1 - p_t)^γ * log(p_t)
    
    Key properties:
    - Down-weights easy examples (high p_t) where model is already confident
    - Focuses learning on hard examples (low p_t) that the model struggles with
    - Perfect for class collapse where model ignores minority classes
    
    In our case: The model predicts Strong Up (34.7%) instead of Weak Up (3.8%)
    because Strong Up is "easier". Focal loss penalizes these easy examples
    and forces attention to the hard Weak Up distinctions.
    
    Reference: "Focal Loss for Dense Object Detection" (Lin et al., 2017)
    """
    
    def __init__(
        self, 
        weight: Optional[torch.Tensor] = None,
        gamma: float = 2.0,
        reduction: str = 'mean',
        label_smoothing: float = 0.1
    ):
        """
        Args:
            weight: Class weights tensor [num_classes]
            gamma: Focusing parameter. Higher = more focus on hard examples.
                   γ=0 is equivalent to CrossEntropyLoss
                   γ=2 is typical for object detection
            reduction: 'none', 'mean', or 'sum'
            label_smoothing: Prevents overconfidence, helps generalization
        """
        super().__init__()
        self.weight = weight
        self.gamma = gamma
        self.reduction = reduction
        self.label_smoothing = label_smoothing
        
    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            logits: Raw model outputs [batch, num_classes]
            targets: Class labels [batch]
        
        Returns:
            Focal loss value
        """
        num_classes = logits.size(-1)
        
        # Apply label smoothing
        # Converts hard labels to soft: [0,0,1,0,0] -> [0.02, 0.02, 0.92, 0.02, 0.02]
        if self.label_smoothing > 0:
            with torch.no_grad():
                smooth_targets = torch.zeros_like(logits)
                smooth_targets.fill_(self.label_smoothing / (num_classes - 1))
                smooth_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)
        
        # Compute softmax probabilities
        probs = torch.softmax(logits, dim=-1)
        
        # Get probability for true class: p_t
        p_t = probs.gather(1, targets.unsqueeze(1)).squeeze(1)
        
        # Compute focal weight: (1 - p_t)^gamma
        # When p_t is high (easy example), weight is low
        # When p_t is low (hard example), weight is high
        focal_weight = (1 - p_t) ** self.gamma
        
        # Compute cross-entropy (with or without label smoothing)
        if self.label_smoothing > 0:
            # Log softmax for numerical stability
            log_probs = torch.log_softmax(logits, dim=-1)
            ce_loss = -(smooth_targets * log_probs).sum(dim=-1)
        else:
            ce_loss = nn.functional.cross_entropy(
                logits, targets, weight=self.weight, reduction='none'
            )
        
        # Apply focal weight
        focal_loss = focal_weight * ce_loss
        
        # Apply class weights if provided (for label smoothing path)
        if self.weight is not None and self.label_smoothing > 0:
            class_weight = self.weight[targets]
            focal_loss = focal_loss * class_weight
        
        # Reduce
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss


class RankingMSELoss(nn.Module):
    """
    Loss that prevents mode collapse using RANKING + NOISE INJECTION.
    
    WHY ALL PREVIOUS LOSSES FAILED:
    MSE with noisy targets INHERENTLY encourages constant predictions!
    The optimal MSE prediction for noisy data is the conditional mean.
    If features aren't predictive, optimal = unconditional mean (constant).
    
    SOLUTION: Two-pronged attack:
    
    1. NOISE INJECTION: Add Gaussian noise to predictions during training.
       This breaks symmetry and forces the model to explore output space.
       The noise is scaled to match target std, so the model MUST learn
       to output values in the correct range to minimize loss.
    
    2. RANKING LOSS: Instead of just matching values, reward correct ORDERING.
       If target_i > target_j, then pred_i should be > pred_j.
       This has gradient even when predictions are constant!
       
       margin_loss = max(0, margin - (pred_i - pred_j)) when target_i > target_j
       
       At collapse (pred_i = pred_j), gradient = -1 (pushes pred_i UP, pred_j DOWN)
    """
    
    def __init__(self, ranking_weight: float = 2.0):
        """
        Args:
            ranking_weight: Weight for ranking loss component
        """
        super().__init__()
        self.ranking_weight = ranking_weight
        
    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        predictions = predictions.squeeze()
        targets = targets.squeeze()
        batch_size = predictions.size(0)
        
        # 1. Raw MSE - learns magnitude
        mse_loss = nn.functional.mse_loss(predictions, targets)
        
        # 2. RANKING LOSS - learns relative ordering
        # Pairwise ranking loss
        ranking_loss = torch.tensor(0.0, device=predictions.device)
        if batch_size > 1:
            # Sample random pairs (efficient implementation)
            # Compare i with i+1 (cyclic)
            idx1 = torch.arange(batch_size, device=predictions.device)
            idx2 = (idx1 + 1) % batch_size
            
            target_diff = targets[idx1] - targets[idx2]
            pred_diff = predictions[idx1] - predictions[idx2]
            
            # Only penalize if signs don't match direction
            # If target_diff > 0, we want pred_diff > 0
            # Loss = ReLU(-sign(target_diff) * pred_diff)
            signs = torch.sign(target_diff)
            # Ignore small differences to reduce noise
            mask = (torch.abs(target_diff) > 0.0001).float()
            
            ranking_loss = (torch.nn.functional.relu(-signs * pred_diff) * mask).mean()
        
        # 3. VARIANCE LOSS - Force scale matching (Robust)
        pred_std = predictions.std() + 1e-6
        target_std = targets.std().detach() + 1e-6
        variance_loss = (pred_std - target_std) ** 2

        # 4. MEAN LOSS - Force centering (Anti-bias)
        pred_mean = predictions.mean()
        target_mean = targets.mean().detach()
        mean_loss = (pred_mean - target_mean) ** 2
        
        # Combined loss
        # Variance and Mean losses are critical for avoiding collapse
        total_loss = mse_loss + \
                     self.ranking_weight * ranking_loss + \
                     10.0 * variance_loss + \
                     10.0 * mean_loss
        
        return total_loss


def compute_class_weights(
    labels: np.ndarray,
    num_classes: int,
    device: torch.device
) -> torch.Tensor:
    """
    Compute normalized class weights to counter class imbalance.

    Uses inverse frequency weighting normalized to mean=1.

    Args:
        labels: Training labels
        num_classes: Number of classes
        device: Torch device
    """
    counts = np.bincount(labels.astype(int), minlength=num_classes).astype(np.float32)
    total = counts.sum()

    if total == 0:
        return torch.ones(num_classes, device=device, dtype=torch.float32)

    # Standard inverse frequency weights
    weights = np.where(counts > 0, total / (num_classes * counts), 0.0)

    # Normalize to mean=1
    mean_weight = weights.mean() if weights.mean() > 0 else 1.0
    weights = weights / mean_weight

    return torch.tensor(weights, device=device, dtype=torch.float32)


class AnalystTrainer:
    """
    Trainer class for the Market Analyst model.

    Features:
    - AdamW optimizer with weight decay
    - DirectionalLoss for robustness and direction accuracy
    - Early stopping
    - Memory-efficient batch processing
    - Checkpoint saving
    - Comprehensive logging and metrics
    - Training visualizations
    """

    def __init__(
        self,
        model: MarketAnalyst,
        device: torch.device,
        learning_rate: float = 1e-3,  # Increased from 1e-4 to escape mode collapse
        weight_decay: float = 1e-5,
        patience: int = 10,
        cache_clear_interval: int = 50,
        log_dir: Optional[str] = None,
        visualize: bool = True,
        num_classes: int = 5,
        class_weights: Optional[torch.Tensor] = None,
        up_classes: Tuple[int, ...] = (3, 4),
        down_classes: Tuple[int, ...] = (0, 1),
        class_names: Optional[List[str]] = None,
        class_meta: Optional[Dict[str, float]] = None
    ):
        """
        Args:
            model: MarketAnalyst model
            device: Torch device
            learning_rate: Learning rate
            weight_decay: AdamW weight decay
            patience: Early stopping patience
            cache_clear_interval: Clear MPS cache every N batches
            log_dir: Directory for logs and visualizations
            visualize: Whether to create visualizations
            num_classes: Number of discrete return classes
            class_weights: Optional class weighting tensor for CrossEntropy
            up_classes: Classes considered bullish for direction metrics
            down_classes: Classes considered bearish for direction metrics
            class_names: Optional human-readable class names (len = num_classes)
            class_meta: Optional metadata about class thresholds/std
        """
        self.model = model.to(device)
        self.device = device
        self.patience = patience
        self.cache_clear_interval = cache_clear_interval
        self.visualize = visualize
        self.log_dir = Path(log_dir) if log_dir else None
        self.num_classes = num_classes
        self.up_classes = up_classes
        self.down_classes = down_classes
        self.class_meta = class_meta or {}

        # Support both 3-class and 5-class schemes
        default_class_names_3 = ["Down", "Neutral", "Up"]
        default_class_names_5 = ["Strong Down", "Weak Down", "Neutral", "Weak Up", "Strong Up"]
        default_class_names = default_class_names_3 if num_classes == 3 else default_class_names_5
        self.class_names = class_names or default_class_names[:num_classes]
        if len(self.class_names) < num_classes:
            missing = [f"Class {i}" for i in range(len(self.class_names), num_classes)]
            self.class_names = self.class_names + missing

        # Setup logging
        if self.log_dir:
            self.log_dir.mkdir(parents=True, exist_ok=True)

        self.training_logger = TrainingLogger(
            name="analyst_training",
            log_dir=str(self.log_dir) if self.log_dir else None,
            log_every_n_batches=50,
            verbose=True
        )

        # Setup visualizer
        if self.visualize and self.log_dir:
            self.visualizer = TrainingVisualizer(save_dir=str(self.log_dir / "plots"))
        else:
            self.visualizer = None

        # Optimizer
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )

        # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5
        )

        if class_weights is not None:
            class_weights = class_weights.to(device)

        # FocalLoss to address class oscillation issue
        # - gamma=2.0: Down-weights easy examples, focuses on hard Up/Down distinctions
        # - label_smoothing=0.1: Softens noisy boundaries in financial data
        # This replaces CrossEntropyLoss which caused oscillation between Up/Down bias
        self.criterion = FocalLoss(
            weight=class_weights,
            gamma=2.0,
            label_smoothing=0.1
        )

        # Training history
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        self.train_direction_accs = []
        self.val_direction_accs = []
        self.train_macro_f1s = []
        self.val_macro_f1s = []
        self.learning_rates = []
        self.grad_norms = []
        self.memory_usage = []
        self.best_val_loss = float('inf')
        self.epochs_without_improvement = 0

        # Batch-level tracking
        self.batch_losses = []
        self.batch_grad_norms = []

    def train_epoch(
        self,
        train_loader: DataLoader,
        epoch: int,
        total_epochs: int
    ) -> Tuple[float, float, float, float]:
        """
        Train for one epoch with detailed metrics.

        Returns:
            Tuple of (avg_loss, accuracy, direction_accuracy, macro_f1)
        """
        self.model.train()
        self.criterion.train()  # Enable noise injection
        total_loss = 0.0
        n_batches = len(train_loader)

        # Metrics tracker for the epoch
        metrics_tracker = MetricsTracker(
            task_type="classification",
            num_classes=self.num_classes,
            up_classes=self.up_classes,
            down_classes=self.down_classes
        )

        self.training_logger.start_epoch(epoch, total_epochs)

        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{total_epochs}")
        for batch_idx, (x_15m, x_1h, x_4h, targets) in enumerate(pbar):
            batch_start = time.time()

            # Move to device
            x_15m = x_15m.to(self.device)
            x_1h = x_1h.to(self.device)
            x_4h = x_4h.to(self.device)
            targets = targets.to(self.device)

            # Forward pass
            self.optimizer.zero_grad()
            _, logits = self.model(x_15m, x_1h, x_4h)
            loss = self.criterion(logits, targets)

            # Backward pass
            loss.backward()

            # Compute gradient norm before clipping
            grad_norm = compute_gradient_norm(self.model)
            self.batch_grad_norms.append(grad_norm)

            # Clip gradients
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()

            # Track metrics
            loss_val = loss.item()
            total_loss += loss_val
            self.batch_losses.append(loss_val)

            # Store predictions for accuracy calculation
            pred_classes = torch.argmax(logits, dim=1)
            metrics_tracker.update(
                pred_classes.detach().cpu().numpy(),
                targets.detach().cpu().numpy(),
                loss_val
            )

            # Get current learning rate
            current_lr = self.optimizer.param_groups[0]['lr']

            # Log batch metrics
            self.training_logger.log_batch(
                batch=batch_idx,
                total_batches=n_batches,
                loss=loss_val,
                grad_norm=grad_norm,
                lr=current_lr
            )

            # Memory cleanup
            if batch_idx % self.cache_clear_interval == 0:
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
                gc.collect()

            # Update progress bar
            pbar.set_postfix({
                'loss': f'{loss_val:.6f}',
                'grad': f'{grad_norm:.4f}',
                'lr': f'{current_lr:.2e}'
            })

            # Clean up batch tensors
            del x_15m, x_1h, x_4h, targets, logits, pred_classes, loss

        # Compute epoch metrics
        epoch_metrics = metrics_tracker.compute()
        avg_loss = total_loss / n_batches
        direction_acc = epoch_metrics.get('direction_accuracy', 0.0)

        accuracy = epoch_metrics.get('accuracy', 0.0)
        macro_f1 = epoch_metrics.get('macro_f1', 0.0)

        return avg_loss, accuracy, direction_acc, macro_f1

    @torch.no_grad()
    def validate(
        self,
        val_loader: DataLoader
    ) -> Tuple[float, float, float, np.ndarray, np.ndarray]:
        """
        Validate the model with detailed metrics.

        Returns:
            Tuple of (avg_loss, accuracy, direction_accuracy, predictions, targets)
        """
        self.model.eval()
        total_loss = 0.0
        n_batches = 0

        # Collect all predictions and targets
        all_predictions = []
        all_targets = []
        metrics_tracker = MetricsTracker(
            task_type="classification",
            num_classes=self.num_classes,
            up_classes=self.up_classes,
            down_classes=self.down_classes
        )

        for x_15m, x_1h, x_4h, targets in val_loader:
            x_15m = x_15m.to(self.device)
            x_1h = x_1h.to(self.device)
            x_4h = x_4h.to(self.device)
            targets = targets.to(self.device)

            _, logits = self.model(x_15m, x_1h, x_4h)
            loss = self.criterion(logits, targets)

            total_loss += loss.item()
            n_batches += 1

            pred_classes = torch.argmax(logits, dim=1)

            all_predictions.append(pred_classes.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
            metrics_tracker.update(pred_classes.cpu().numpy(), targets.cpu().numpy(), loss.item())

            del x_15m, x_1h, x_4h, targets, logits, pred_classes

        # Concatenate all predictions and targets
        all_predictions = np.concatenate(all_predictions) if len(all_predictions) > 0 else np.array([])
        all_targets = np.concatenate(all_targets) if len(all_targets) > 0 else np.array([])

        # Calculate metrics
        avg_loss = total_loss / max(n_batches, 1)
        class_metrics = metrics_tracker.compute()

        accuracy = class_metrics.get('accuracy', 0.0)
        direction_acc = class_metrics.get('direction_accuracy', 0.0)

        return avg_loss, accuracy, direction_acc, all_predictions, all_targets

    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        max_epochs: int = 100,
        save_path: Optional[str] = None
    ) -> Dict:
        """
        Full training loop with early stopping, logging, and visualizations.

        Args:
            train_loader: Training data loader
            val_loader: Validation data loader
            max_epochs: Maximum epochs
            save_path: Path to save best model

        Returns:
            Training history
        """
        # Count model parameters
        total_params = sum(p.numel() for p in self.model.parameters())

        self.training_logger.start_training(max_epochs, total_params)
        logger.info(f"Starting training for up to {max_epochs} epochs")
        logger.info(f"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}")

        for epoch in range(1, max_epochs + 1):
            # Train
            train_loss, train_acc, train_dir_acc, train_macro_f1 = self.train_epoch(
                train_loader, epoch, max_epochs
            )
            self.train_losses.append(train_loss)
            self.train_accs.append(train_acc)
            self.train_direction_accs.append(train_dir_acc)
            self.train_macro_f1s.append(train_macro_f1)

            # Validate
            val_loss, val_acc, val_dir_acc, val_preds, val_targets = self.validate(val_loader)
            self.val_losses.append(val_loss)
            self.val_accs.append(val_acc)
            self.val_direction_accs.append(val_dir_acc)

            # Detailed validation metrics
            class_metrics = calculate_classification_metrics(
                val_preds,
                val_targets,
                num_classes=self.num_classes,
                up_classes=self.up_classes,
                down_classes=self.down_classes
            )
            self.val_macro_f1s.append(class_metrics.macro_f1)

            # Get current learning rate
            current_lr = self.optimizer.param_groups[0]['lr']
            self.learning_rates.append(current_lr)

            # Compute average gradient norm for epoch
            recent_grad_norms = self.batch_grad_norms[-len(train_loader):]
            avg_grad_norm = np.mean(recent_grad_norms) if recent_grad_norms else 0.0
            self.grad_norms.append(avg_grad_norm)

            # Update scheduler
            self.scheduler.step(val_loss)

            # Log epoch summary
            self.training_logger.log_epoch(
                epoch=epoch,
                train_loss=train_loss,
                val_loss=val_loss,
                train_acc=train_acc,
                val_acc=val_acc,
                train_direction_acc=train_dir_acc,
                val_direction_acc=val_dir_acc,
                lr=current_lr,
                grad_norm=avg_grad_norm,
                extra_metrics={
                    'train_macro_f1': train_macro_f1,
                    'val_macro_f1': class_metrics.macro_f1,
                    'val_up_recall': class_metrics.up_recall,
                    'val_down_recall': class_metrics.down_recall,
                    'val_neutral_recall': class_metrics.neutral_recall
                }
            )

            # Log class distribution for debugging
            pred_counts = np.bincount(val_preds.astype(int), minlength=self.num_classes)
            tgt_counts = np.bincount(val_targets.astype(int), minlength=self.num_classes)
            total_pred = pred_counts.sum() if pred_counts.sum() > 0 else 1
            total_tgt = tgt_counts.sum() if tgt_counts.sum() > 0 else 1
            logger.info("  Class distribution (pred | true):")
            for idx, name in enumerate(self.class_names):
                pred_pct = pred_counts[idx] / total_pred * 100
                tgt_pct = tgt_counts[idx] / total_tgt * 100
                logger.info(f"    {idx} ({name}): pred {pred_pct:5.1f}% | true {tgt_pct:5.1f}%")

            # Log sample predictions vs targets
            if epoch % 10 == 0 or epoch == 1:
                self.training_logger.log_validation_details(
                    val_preds,
                    val_targets,
                    task_type="classification",
                    class_names=self.class_names
                )

            # Check for improvement
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.epochs_without_improvement = 0

                # Save best model
                if save_path:
                    self.save_checkpoint(save_path, epoch, is_best=True)
            else:
                self.epochs_without_improvement += 1

            # Create epoch visualization
            if self.visualizer and epoch % 5 == 0:
                # Get training predictions for visualization
                train_preds, train_tgts = self._get_train_predictions(train_loader)

                metrics = {
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'train_acc': train_acc,
                    'val_acc': val_acc,
                    'train_dir_acc': train_dir_acc,
                    'val_dir_acc': val_dir_acc,
                    'train_macro_f1': train_macro_f1,
                    'val_macro_f1': class_metrics.macro_f1,
                    'learning_rate': current_lr,
                    'grad_norm': avg_grad_norm
                }

                self.visualizer.plot_epoch_summary(
                    epoch=epoch,
                    train_predictions=train_preds,
                    train_targets=train_tgts,
                    val_predictions=val_preds,
                    val_targets=val_targets,
                    metrics=metrics,
                    save_name=f'epoch_{epoch:03d}_summary.png',
                    task_type="classification",
                    class_names=self.class_names,
                    num_classes=self.num_classes
                )
                self.visualizer.close_all()

            # Early stopping
            if self.epochs_without_improvement >= self.patience:
                logger.info(f"Early stopping at epoch {epoch}")
                self.training_logger.end_training(reason="early_stopped")
                break

            # Memory cleanup
            gc.collect()
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()

        else:
            self.training_logger.end_training(reason="completed")

        # Create final visualizations
        if self.visualizer:
            history = self.get_history()
            self.visualizer.plot_training_curves(
                history,
                title="Market Analyst Training History",
                save_name="training_curves.png"
            )

            # Direction confusion matrix
            _, _, _, final_preds, final_targets = self.validate(val_loader)
            self.visualizer.plot_direction_confusion(
                final_preds, final_targets,
                title="Final Direction Classification Performance",
                save_name="direction_confusion.png",
                task_type="classification",
                up_classes=self.up_classes,
                down_classes=self.down_classes
            )

            self.visualizer.plot_predictions_vs_targets(
                final_preds, final_targets,
                title="Final Predictions vs Targets",
                save_name="predictions_vs_targets.png",
                task_type="classification",
                class_names=self.class_names
            )

            # Learning dynamics
            if len(self.batch_losses) > 0:
                self.visualizer.plot_learning_dynamics(
                    self.batch_losses,
                    self.batch_grad_norms,
                    title="Learning Dynamics",
                    save_name="learning_dynamics.png"
                )

            self.visualizer.close_all()

        return self.get_history()

    @torch.no_grad()
    def _get_train_predictions(
        self,
        train_loader: DataLoader,
        max_batches: int = 50
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Get a sample of training predictions for visualization."""
        self.model.eval()
        predictions = []
        targets = []

        for batch_idx, (x_15m, x_1h, x_4h, tgt) in enumerate(train_loader):
            if batch_idx >= max_batches:
                break

            x_15m = x_15m.to(self.device)
            x_1h = x_1h.to(self.device)
            x_4h = x_4h.to(self.device)

            _, logits = self.model(x_15m, x_1h, x_4h)
            pred_classes = torch.argmax(logits, dim=1)
            predictions.append(pred_classes.cpu().numpy())
            targets.append(tgt.numpy())

        self.model.train()
        return np.concatenate(predictions), np.concatenate(targets)

    def get_history(self) -> Dict[str, List[float]]:
        """Get training history as dictionary."""
        return {
            'train_loss': self.train_losses,
            'val_loss': self.val_losses,
            'train_acc': self.train_accs,
            'val_acc': self.val_accs,
            'train_direction_acc': self.train_direction_accs,
            'val_direction_acc': self.val_direction_accs,
            'train_macro_f1': self.train_macro_f1s,
            'val_macro_f1': self.val_macro_f1s,
            'learning_rate': self.learning_rates,
            'grad_norm': self.grad_norms,
            'best_val_loss': self.best_val_loss,
            'epochs_trained': len(self.train_losses)
        }

    def save_checkpoint(
        self,
        path: str,
        epoch: int,
        is_best: bool = False
    ):
        """Save model checkpoint."""
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)

        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accs': self.train_accs,
            'val_accs': self.val_accs,
            'train_direction_accs': self.train_direction_accs,
            'val_direction_accs': self.val_direction_accs,
            'best_val_loss': self.best_val_loss,
            'config': {
            'd_model': self.model.d_model,
            'nhead': self.model.nhead,
            'num_layers': self.model.num_layers,
            'dim_feedforward': self.model.dim_feedforward,
            'dropout': self.model.dropout,
            'context_dim': self.model.context_dim,
            'num_classes': self.model.num_classes,
            'up_classes': self.up_classes,
            'down_classes': self.down_classes,
            'class_names': self.class_names,
        }
        }

        filename = 'best.pt' if is_best else f'epoch_{epoch}.pt'
        torch.save(checkpoint, path / filename)
        logger.info(f"Saved checkpoint to {path / filename}")


def train_analyst(
    df_15m: pd.DataFrame,
    df_1h: pd.DataFrame,
    df_4h: pd.DataFrame,
    feature_cols: List[str],
    save_path: str,
    config: Optional[object] = None,
    device: Optional[torch.device] = None,
    visualize: bool = True
) -> Tuple[MarketAnalyst, Dict]:
    """
    Main function to train the Market Analyst.

    Args:
        df_15m: 15-minute DataFrame with features
        df_1h: 1-hour DataFrame with features
        df_4h: 4-hour DataFrame with features
        feature_cols: Feature columns to use
        save_path: Path to save model
        config: AnalystConfig object
        device: Torch device
        visualize: Whether to create visualizations

    Returns:
        Tuple of (trained model, training history)
    """
    # Class names depend on num_classes from config
    class_names_3 = [
        "Down (< -0.5σ)",
        "Neutral (-0.5σ to +0.5σ)",
        "Up (> +0.5σ)"
    ]
    class_names_5 = [
        "Strong Down (<-0.5σ)",
        "Weak Down (-0.5σ to -0.1σ)",
        "Neutral (-0.1σ to +0.1σ)",
        "Weak Up (+0.1σ to +0.5σ)",
        "Strong Up (> +0.5σ)"
    ]

    # Default configuration
    if config is None:
        from config.settings import Config
        config = Config().analyst

    num_classes = config.num_classes if hasattr(config, 'num_classes') else 5
    class_names = class_names_3 if num_classes == 3 else class_names_5
    if len(class_names) < num_classes:
        class_names += [f"Class {i}" for i in range(len(class_names), num_classes)]

    if device is None:
        if torch.backends.mps.is_available():
            device = torch.device('mps')
        else:
            device = torch.device('cpu')

    logger.info(f"Training on device: {device}")

    # Create target
    logger.info("Creating smoothed target...")
    target = create_smoothed_target(
        df_15m,
        future_window=config.future_window if hasattr(config, 'future_window') else 12,
        smooth_window=config.smooth_window if hasattr(config, 'smooth_window') else 12
    )

    # Log target statistics
    valid_target = target.dropna()
    logger.info(f"Target stats: mean={valid_target.mean():.6f}, std={valid_target.std():.6f}, "
               f"min={valid_target.min():.6f}, max={valid_target.max():.6f}")
    logger.info(f"Target distribution: {(valid_target > 0).mean()*100:.1f}% positive, "
               f"{(valid_target < 0).mean()*100:.1f}% negative")

    # Convert to classification labels
    thresholds = config.class_std_thresholds if hasattr(config, 'class_std_thresholds') else (-0.5, -0.5, 0.5, 0.5)
    class_labels, class_meta = create_return_classes(
        target,
        class_std_thresholds=thresholds
    )
    label_counts = class_labels.value_counts(dropna=True).sort_index()
    total_labels = label_counts.sum() if label_counts.sum() > 0 else 1

    logger.info("Class boundaries (scaled returns):")
    if num_classes == 3:
        logger.info(f"  Down     (<): {class_meta.get('down_threshold', class_meta['strong_down_threshold']):.6f}")
        logger.info(f"  Neutral  (<=): {class_meta.get('up_threshold', class_meta['strong_up_threshold']):.6f}")
        logger.info(f"  Up       (>): {class_meta.get('up_threshold', class_meta['strong_up_threshold']):.6f}")
    else:
        logger.info(f"  Strong Down (<): {class_meta['strong_down_threshold']:.6f}")
        logger.info(f"  Weak Down  (<): {class_meta['weak_down_threshold']:.6f}")
        logger.info(f"  Neutral    (<=): {class_meta['weak_up_threshold']:.6f}")
        logger.info(f"  Strong Up  (>): {class_meta['strong_up_threshold']:.6f}")
    logger.info("Class distribution (overall):")
    for idx, count in label_counts.items():
        name = class_names[int(idx)] if int(idx) < len(class_names) else f"Class {int(idx)}"
        pct = count / total_labels * 100
        logger.info(f"  {int(idx)} ({name}): {count} samples ({pct:.1f}%)")

    # Add Market Sessions
    logger.info("Adding market session features...")
    df_15m = add_market_sessions(df_15m)
    df_1h = add_market_sessions(df_1h)
    df_4h = add_market_sessions(df_4h)

    # Add Structure Features (BOS/CHoCH)
    logger.info("Adding structure features (BOS/CHoCH)...")
    for df in [df_15m, df_1h, df_4h]:
        f_high, f_low = detect_fractals(df)
        struct_df = detect_structure_breaks(df, f_high, f_low)
        for col in struct_df.columns:
            df[col] = struct_df[col]

    # Update feature columns if not already included
    session_cols = ['session_asian', 'session_london', 'session_ny']
    struct_cols = ['bos_bullish', 'bos_bearish', 'choch_bullish', 'choch_bearish']
    
    for col in session_cols + struct_cols:
        if col not in feature_cols:
            feature_cols.append(col)
            
    # Create dataset
    logger.info("Creating dataset...")
    dataset = MultiTimeframeDataset(
        df_15m, df_1h, df_4h,
        feature_cols, target, class_labels,
        lookback_15m=config.lookback_15m if hasattr(config, 'lookback_15m') else 48,
        lookback_1h=config.lookback_1h if hasattr(config, 'lookback_1h') else 24,
        lookback_4h=config.lookback_4h if hasattr(config, 'lookback_4h') else 12
    )

    # Split into train/validation using CHRONOLOGICAL split (NOT random!)
    # CRITICAL: Random splits cause look-ahead bias in time series data.
    # The model would train on "Tuesday" and test on "Monday", memorizing the future.
    train_size = int(0.85 * len(dataset))
    val_size = len(dataset) - train_size

    # Use Subset with sequential indices for chronological split
    train_indices = list(range(0, train_size))
    val_indices = list(range(train_size, len(dataset)))

    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    val_dataset = torch.utils.data.Subset(dataset, val_indices)

    logger.info(f"Train size: {train_size} (indices 0-{train_size-1})")
    logger.info(f"Val size: {val_size} (indices {train_size}-{len(dataset)-1})")
    logger.info("Using CHRONOLOGICAL split (train on past, validate on future)")

    # Class weights and split distributions
    valid_label_array = class_labels.values[dataset.valid_indices]
    train_label_array = valid_label_array[:train_size].astype(int)
    val_label_array = valid_label_array[train_size:].astype(int)

    class_weights = compute_class_weights(train_label_array, num_classes, device)
    logger.info(f"Class weights (normalized): {class_weights.cpu().numpy()}")

    def _log_split_distribution(name: str, labels: np.ndarray):
        counts = np.bincount(labels, minlength=num_classes)
        total = counts.sum() if counts.sum() > 0 else 1
        parts = []
        for idx, count in enumerate(counts):
            class_name = class_names[idx] if idx < len(class_names) else f"Class {idx}"
            parts.append(f"{idx} {class_name}: {count} ({count/total*100:.1f}%)")
        logger.info(f"{name} class mix: " + " | ".join(parts))

    _log_split_distribution("Train", train_label_array)
    _log_split_distribution("Val", val_label_array)

    # Create data loaders
    batch_size = config.batch_size if hasattr(config, 'batch_size') else 32

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # MPS doesn't support multiprocessing well
        pin_memory=False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )

    # Create model
    feature_dims = {
        '15m': len(feature_cols),
        '1h': len(feature_cols),
        '4h': len(feature_cols)
    }

    model = create_analyst(feature_dims, config, device)
    logger.info(f"Created MarketAnalyst with {sum(p.numel() for p in model.parameters()):,} parameters")

    # Log model architecture
    logger.info(f"Model config: d_model={model.d_model}, context_dim={model.context_dim}")

    # Create trainer
    trainer = AnalystTrainer(
        model=model,
        device=device,
        learning_rate=config.learning_rate if hasattr(config, 'learning_rate') else 1e-4,
        weight_decay=config.weight_decay if hasattr(config, 'weight_decay') else 1e-5,
        patience=config.patience if hasattr(config, 'patience') else 10,
        log_dir=save_path,
        visualize=visualize,
        num_classes=num_classes,
        class_weights=class_weights,
        up_classes=(2,) if num_classes == 3 else (3, 4),
        down_classes=(0,) if num_classes == 3 else (0, 1),
        class_names=class_names,
        class_meta=class_meta
    )

    # Train
    history = trainer.train(
        train_loader,
        val_loader,
        max_epochs=config.max_epochs if hasattr(config, 'max_epochs') else 100,
        save_path=save_path
    )

    # Load best model
    best_path = Path(save_path) / 'best.pt'
    if best_path.exists():
        checkpoint = torch.load(best_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        logger.info(f"Loaded best model from epoch {checkpoint['epoch']}")

    # Final summary
    logger.info("=" * 70)
    logger.info("TRAINING COMPLETE")
    logger.info("=" * 70)
    logger.info(f"Best validation loss: {history['best_val_loss']:.6f}")
    logger.info(f"Final train accuracy: {history['train_acc'][-1]*100:.2f}%")
    logger.info(f"Final val accuracy: {history['val_acc'][-1]*100:.2f}%")
    logger.info(f"Final train direction acc: {history['train_direction_acc'][-1]*100:.2f}%")
    logger.info(f"Final val direction acc: {history['val_direction_acc'][-1]*100:.2f}%")
    logger.info(f"Total epochs trained: {history['epochs_trained']}")
    logger.info("=" * 70)

    return model, history


if __name__ == '__main__':
    # Example usage
    print("Use this module via: python -m src.training.train_analyst")
    print("Or import and call train_analyst() function")
